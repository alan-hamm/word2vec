{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This code was written using CDC AI Chatbot. A variety of prompts were used, including questions and prompts to \n",
    "    correct bugs, memory issues(ie too little resources available), generate comments, etc.\n",
    "\n",
    "maintenance: alan hamm(pqn7)\n",
    "apr 2024\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis.gensim\n",
    "import torch  # PyTorch library for deep learning and GPU acceleration\n",
    "from torch.utils.data import DataLoader  # Provides an iterator over a dataset for efficient batch processing\n",
    "from tqdm import tqdm  # Creates progress bars to visualize the progress of loops or tasks\n",
    "from sklearn.feature_extraction.text import CountVectorizer  # Converts text documents into numerical representations\n",
    "from sklearn.decomposition import LatentDirichletAllocation  # Implements Latent Dirichlet Allocation (LDA) for topic modeling\n",
    "from gensim.models import LdaModel  # Implements LDA for topic modeling using the Gensim library\n",
    "from gensim.models import LdaMulticore\n",
    "from gensim.corpora import Dictionary  # Represents a collection of text documents as a bag-of-words corpus\n",
    "from gensim.models import CoherenceModel\n",
    "import gensim\n",
    "import json\n",
    "\n",
    "import os  # Provides functions for interacting with the operating system, such as creating directories\n",
    "import pickle  # Allows objects to be serialized and deserialized to/from disk\n",
    "import itertools  # Provides various functions for efficient iteration and combination of elements\n",
    "import numpy as np  # Library for numerical computing in Python, used for array operations and calculations\n",
    "from time import time  # Measures the execution time of code snippets or functions\n",
    "import pprint as pp  # Pretty-printing library, used here to format output in a readable way\n",
    "import multiprocessing\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from scipy.sparse import csr_matrix\n",
    "#from scipy.sparse.linalg import triu\n",
    "\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "\n",
    "import dask\n",
    "import dask\n",
    "from dask.distributed import Client, LocalCluster #, LocalCUDACluster\n",
    "from dask.diagnostics import ProgressBar\n",
    "import dask.bag as db\n",
    "import torch\n",
    "import pickle\n",
    "import itertools\n",
    "from gensim.models import Word2Vec\n",
    "import cupy as cp\n",
    "import webbrowser\n",
    "from torchtext.vocab import GloVe\n",
    "from gensim.models import KeyedVectors\n",
    "import torchtext.vocab as vocab\n",
    "import logging\n",
    "from gensim.models.callbacks import PerplexityMetric, ConvergenceMetric, CoherenceMetric\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dask dashboard throws deprecation warnings w.r.t. Bokeh\n",
    "import warnings\n",
    "from bokeh.util.deprecation import BokehDeprecationWarning\n",
    "\n",
    "# Disable Bokeh deprecation warnings\n",
    "warnings.filterwarnings(\"ignore\", category=BokehDeprecationWarning)\n",
    "\n",
    "#BokehDeprecationWarning: 'circle() method with size value' was deprecated in Bokeh 3.4.0 and will be removed, use 'scatter(size=...) instead' instead.\n",
    "#BokehDeprecationWarning: 'circle() method with size value' was deprecated in Bokeh 3.4.0 and will be removed, use 'scatter(size=...) instead' instead.\n",
    "#BokehDeprecationWarning: 'square() method' was deprecated in Bokeh 3.4.0 and will be removed, use \"scatter(marker='square', ...) instead\" instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the range of number of topics for LDA and step size\n",
    "start_topics = 1\n",
    "end_topics = 3\n",
    "step_size = 2\n",
    "\n",
    "MIN_YEAR = 2010\n",
    "MAX_YEAR = 2020\n",
    "\n",
    "# Specify output directories for log file, model outputs, and images generated.\n",
    "log_dir = \"C:/_harvester/data/lda-models/2010s_html/\"\n",
    "model_dir = \"C:/_harvester/data/lda-models/2010s_html/lda-models/\"\n",
    "image_dir = \"C:/_harvester/data/lda-models/2010s_html/visuals/\"\n",
    "\n",
    "# Create directories if they don't exist.\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "os.makedirs(image_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of available GPUs: 1\n",
      "\n",
      "GPU Device 0 Properties:\n",
      "Device Name: NVIDIA RTX A3000 12GB Laptop GPU\n",
      "Total Memory: 12.00 GB\n",
      "Multiprocessor Count: 32\n",
      "CUDA Capability Major Version: 8\n",
      "CUDA Capability Minor Version: 6\n",
      "PyTorch is using the GPU.\n",
      "CUDA Version: 12.1\n"
     ]
    }
   ],
   "source": [
    "# Check if CUDA is available\n",
    "if torch.cuda.is_available():\n",
    "    # Get the number of available GPUs\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    print(f\"Number of available GPUs: {num_gpus}\")\n",
    "    \n",
    "    for i in range(num_gpus):\n",
    "        # Get the properties of each GPU device\n",
    "        gpu_properties = torch.cuda.get_device_properties(i)\n",
    "        \n",
    "        print(f\"\\nGPU Device {i} Properties:\")\n",
    "        print(f\"Device Name: {gpu_properties.name}\")\n",
    "        print(f\"Total Memory: {gpu_properties.total_memory / 1024**3:.2f} GB\")\n",
    "        print(f\"Multiprocessor Count: {gpu_properties.multi_processor_count}\")\n",
    "        print(f\"CUDA Capability Major Version: {gpu_properties.major}\")\n",
    "        print(f\"CUDA Capability Minor Version: {gpu_properties.minor}\")\n",
    "else:\n",
    "    print(\"CUDA is not available.\")\n",
    "\n",
    "# Set device to GPU if available, otherwise use CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# verify if CUDA is being used or the CPU\n",
    "if device is not None:\n",
    "    # Check if PyTorch is currently using the GPU\n",
    "    if torch.backends.cudnn.enabled:\n",
    "        print(\"PyTorch is using the GPU.\")\n",
    "        cuda_version = torch.version.cuda\n",
    "        print(\"CUDA Version:\", cuda_version)\n",
    "    else:\n",
    "        print(\"PyTorch is using the CPU.\")\n",
    "else:\n",
    "    print(\"The device is neither using the GPU nor CPU. An error has ocurred.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cores = multiprocessing.cpu_count() - 1 # Count the number of cores in a computer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The parameter `alpha` in Latent Dirichlet Allocation (LDA) represents the concentration parameter of the Dirichlet \n",
    "# prior distribution for the topic-document distribution.\n",
    "# It controls the sparsity of the resulting document-topic distributions.\n",
    "\n",
    "# A lower value of `alpha` leads to sparser distributions, meaning that each document is likely to be associated with fewer topics.\n",
    "# Conversely, a higher value of `alpha` encourages documents to be associated with more topics, resulting in denser distributions.\n",
    "\n",
    "# The choice of `alpha` affects the balance between topic diversity and document specificity in LDA modeling.\n",
    "alpha_values = np.arange(0.01, 1, 0.3).tolist()\n",
    "alpha_values += ['symmetric', 'asymmetric']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Latent Dirichlet Allocation (LDA) topic analysis, the beta parameter represents the concentration \n",
    "# parameter of the Dirichlet distribution used to model the topic-word distribution. It controls the \n",
    "# sparsity of topics by influencing how likely a given word is to be assigned to a particular topic.\n",
    "\n",
    "# A higher value of beta encourages topics to have a more uniform distribution over words, resulting in more \n",
    "# general and diverse topics. Conversely, a lower value of beta promotes sparser topics with fewer dominant words.\n",
    "\n",
    "# The choice of beta can impact the interpretability and granularity of the discovered topics in LDA.\n",
    "beta_values = np.arange(0.01, 1, 0.3).tolist()\n",
    "beta_values += ['symmetric']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your dataset as a list of a list of tokenized sentences or load data from a file\n",
    "def get_texts_out(year):\n",
    "    year = int(year)\n",
    "    with open(f\"C:/_harvester/data/tokenized-sentences/10s/{year}-tokenized_sents-w-bigrams.json\", \"rb\") as fp:\n",
    "        texts_out = pickle.load(fp)\n",
    "\n",
    "    print(f\"This is the get_texts_out() function. The size of the return is {len(texts_out)}\")\n",
    "    return texts_out\n",
    "\n",
    "#pp.pprint(get_texts_out(2010))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from typing import List, Optional\n",
    "def coherence_score(X: List[List[str]], n_topics: int, metric: str = 'c_v', vectorizer: Optional[str] = None, glove: Optional[GloVe] = None) -> float:\n",
    "    \"\"\"\n",
    "    Compute the coherence score for a given set of topics and documents.\n",
    "\n",
    "    Args:\n",
    "        X (list): List of documents.\n",
    "        topics (list): List of topic assignments for each document.\n",
    "        metric (str, optional): Coherence metric to use. Defaults to 'c_v'.\n",
    "        vectorizer (str, optional): Vectorizer to use. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        float: Coherence score.\n",
    "\n",
    "    \"\"\"\n",
    "    if vectorizer == 'glove':\n",
    "        # Load pre-trained GloVe embeddings\n",
    "        # load the scattered embedding vectors from across Dask workers\n",
    "        #glove = GloVe(vectors=embedding_vectors)\n",
    "\n",
    "        # Move the embeddings to the GPU device if available\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        if device.type == \"cuda\":\n",
    "            print(\"CUDA is being used by GloVe.\")\n",
    "            print(\"Number of GPUs available:\", torch.cuda.device_count())\n",
    "            print(\"Current GPU:\", torch.cuda.get_device_name(0))\n",
    "            \n",
    "        else:\n",
    "            print(\"CUDA is not being used by GloVe. Using CPU instead.\")\n",
    "\n",
    "        # Convert X to a list of documents\n",
    "        documents = [list(doc) for doc in X]\n",
    "\n",
    "        # Convert documents into numerical representations using GloVe\n",
    "        document_vectors = []\n",
    "        \n",
    "        batch_size = 1000  # Set the batch size\n",
    "        for i in range(0, len(documents), batch_size):\n",
    "            batch_docs = documents[i:i+batch_size]\n",
    "            doc_vectors = [[glove[word] for word in doc] for doc in batch_docs]\n",
    "            document_vectors.extend(doc_vectors)\n",
    "        \n",
    "        X_gpu = []\n",
    "        \n",
    "        num_vectors_to_print = 5  # Number of vectors to print\n",
    "        \n",
    "        for doc_vecs in document_vectors:\n",
    "            doc_gpu = [vec.to(device) for vec in doc_vecs]\n",
    "            X_gpu.append(doc_gpu)\n",
    "            \n",
    "            if num_vectors_to_print > 0:\n",
    "                # Verify if tensors are on GPU and print their devices\n",
    "                for vec in doc_gpu:\n",
    "                    print(\"The vector is on the GPU:\", vec.device)\n",
    "                    num_vectors_to_print -= 1\n",
    "    \n",
    "    else:\n",
    "        print(\"Vectorizer is not set to 'glove'.\")\n",
    "\n",
    "\n",
    "    # Create a dictionary and corpus from the documents\n",
    "    dictionary = Dictionary(X)\n",
    "    corpus = [dictionary.doc2bow(doc) for doc in X]\n",
    "\n",
    "    # Create a topic model using the given topics\n",
    "    topic_model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=n_topics, random_state=42)\n",
    "\n",
    "    # Compute the coherence score using the CoherenceModel\n",
    "    coherence_model = CoherenceModel(model=topic_model, texts=X, dictionary=dictionary, coherence=metric)\n",
    "\n",
    "    return coherence_model.get_coherence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import socket\n",
    "\n",
    "def check_port_in_use(port):\n",
    "    sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "    sock.settimeout(1)  # Set a timeout for the connection attempt\n",
    "    try:\n",
    "        sock.connect(('localhost', port))  # Connect to the specified port\n",
    "        sock.close()  # Close the socket connection\n",
    "        return True  # Port is in use\n",
    "    except ConnectionRefusedError:\n",
    "        return False  # Port is not in use or closed\n",
    "\n",
    "def close_port(port):\n",
    "    if check_port_in_use(port):\n",
    "        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "        sock.settimeout(1)  # Set a timeout for the connection attempt\n",
    "        try:\n",
    "            sock.connect(('localhost', port))  # Connect to the specified port\n",
    "            sock.close()  # Close the socket connection\n",
    "            print(f\"Port {port} is now closed.\")\n",
    "        except ConnectionRefusedError:\n",
    "            print(f\"Port {port} could not be closed.\")\n",
    "    else:\n",
    "        print(f\"Port {port} is already closed or not in use.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The data_generator function is defined as a generator. It opens the specified JSON file (filename) \n",
    "and iterates over its lines using a for loop. Each line is parsed using json.loads() to convert it \n",
    "into a Python object (e.g., dictionary). The yield keyword is used instead of return to create a \n",
    "generator that produces one parsed JSON object at a time.\n",
    "\n",
    "The num_samples variable counts the total number of lines in the JSON file by opening it (open(filename)) \n",
    "and iterating over its lines using a generator expression (sum(1 for _ in open(filename))). This gives \n",
    "us an estimate of how many samples are present in the dataset.\n",
    "\n",
    "The num_train_samples variable calculates the desired number of samples for training based on the provided \n",
    "train_ratio. It multiplies num_samples by train_ratio, converting it to an integer using int().\n",
    "\n",
    "Two empty lists, train_data and eval_data, are initialized to store training and evaluation datasets, respectively.\n",
    "\n",
    "An instance of the `data_generator\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def create_lda_datasets(filename, train_ratio):\n",
    "    # Define a generator function to read data from the file one line at a time\n",
    "    def data_generator():\n",
    "        with open(filename, 'r') as jsonfile:\n",
    "            for line in jsonfile:\n",
    "                yield json.loads(line)  # Yield each line as a parsed JSON object\n",
    "    \n",
    "    num_samples = sum(1 for _ in open(filename))  # Count the total number of lines in the file\n",
    "    num_train_samples = int(num_samples * train_ratio)  # Calculate the number of samples for training\n",
    "    \n",
    "    train_data = []  # Initialize an empty list to store training data\n",
    "    eval_data = []  # Initialize an empty list to store evaluation data\n",
    "    \n",
    "    generator = data_generator()  # Create an instance of the data generator\n",
    "    \n",
    "    # Shuffle and split the data into training and evaluation sets\n",
    "    for i, item in enumerate(generator):\n",
    "        if i < num_train_samples:\n",
    "            train_data.append(item)  # Add item to training data if it's within the desired range\n",
    "        else:\n",
    "            eval_data.append(item)  # Add item to evaluation data if it's outside the desired range\n",
    "    \n",
    "    return train_data, eval_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Dask client was not connected: name 'client' is not defined\n",
      "CUDA is being used by GloVe.\n",
      "Dask client is connected to a scheduler.\n",
      "Dask workers are running.\n",
      "Data format and structure for 2010:\n",
      "Type of min_year_data: <class 'list'>\n",
      "Length of min_year_data: 44137\n",
      "\n",
      "Data format and structure for 2011:\n",
      "Type of current_year_data: <class 'list'>\n",
      "Length of current_year_data: 51985\n",
      "Data lengths are different.\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pqn7\\.conda\\envs\\nlp\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3585: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import dask.delayed\n",
    "import logging\n",
    "logging.basicConfig(filename=f\"C:/_harvester/data/lda-models/2010s_html/model_callback.log\",\n",
    "                                format=\"%(asctime)s:%(levelname)s:%(message)s\",\n",
    "                                level=logging.NOTSET)\n",
    "\n",
    "    \n",
    "if __name__==\"__main__\":\n",
    "    # Create a multiprocessing context using the \"spawn\" method\n",
    "    # This method is recommended for certain platforms, such as Windows or Jupyter Notebook, to avoid conflicts\n",
    "    #ctx = multiprocessing.get_context(\"spawn\")\n",
    "\n",
    "    # Create a Pool of worker processes using the multiprocessing context\n",
    "    # The number of worker processes is cores - 1\n",
    "    # This ensures that one CPU core is left available for other tasks or system operations\n",
    "    #pool = ctx.Pool(cores - 1)\n",
    "\n",
    "    try:\n",
    "        # Check if the Dask client is connected to a scheduler\n",
    "        if client.status == \"running\":\n",
    "            # Close the Dask client\n",
    "            client.close()\n",
    "            print(\"Dask client closed preemptively.\")\n",
    "        else:\n",
    "            print(\"Dask client is not connected to a scheduler.\")\n",
    "    except Exception as e:\n",
    "        print(f\"The Dask client was not connected: {e}\")\n",
    "\n",
    "    # Load the saved embedding vectors from TorchText GloVe library\n",
    "    glove = vocab.Vectors('glove.840B.300d.txt', 'C:/_harvester/GloVe/')\n",
    "\n",
    "    # Get the embedding vectors and vocabulary from TorchText GloVe library\n",
    "    embedding_vectors = glove.vectors\n",
    "\n",
    "    # Move the embeddings to the GPU device if available\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    embedding_vectors = embedding_vectors.to(device)\n",
    "\n",
    "    # Verify if CUDA is being used by checking the device type\n",
    "    if device.type == \"cuda\":\n",
    "        print(\"CUDA is being used by GloVe.\")\n",
    "    else:\n",
    "        print(\"CUDA is not being used by GloVe. Using CPU instead.\")\n",
    "\n",
    "    # Convert embedding vectors to a NumPy array (on CPU)\n",
    "    embedding_array = embedding_vectors.cpu().numpy()\n",
    "\n",
    "    # Dictionary to hold the metrics that are generated\n",
    "    metrics_csv = {\n",
    "        'n_topics': [],\n",
    "        'alpha': [],\n",
    "        'beta': [],\n",
    "        'cv_score': [],\n",
    "        'convergence_score': [],\n",
    "        'log_perplexity': [],\n",
    "        'time_to_complete': []\n",
    "    }\n",
    "\n",
    "    # close the port if it's open\n",
    "    #close_port(8787)\n",
    "    \n",
    "    # Specify the local directory path\n",
    "    DASK_DIR = '/_harvester/tmp-dask-out'\n",
    "\n",
    "    # specify Dask dashboard port\n",
    "    #DASHBOARD_PORT = \"60481\"\n",
    "    \"\"\"\n",
    "    # Set the GPU memory limit\n",
    "    gpu_memory_limit = \"10GB\"\n",
    "    # Set the CUDA_VISIBLE_DEVICES environment variable to specify which GPUs to use\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"  # Specify GPU device IDs\n",
    "    # Create a Dask local cluster with the specified local directory and GPU memory limit\n",
    "    #cluster = LocalCluster(local_directory=DASK_DIR, device_memory_limit=gpu_memory_limit)\n",
    "    cluster = LocalCluster(local_directory=DASK_DIR)\n",
    "    client = Client(cluster)\n",
    "    \"\"\"\n",
    "    # Deploy a Single-Machine Multi-GPU Cluster\n",
    "    # https://medium.com/@aryan.gupta18/end-to-end-recommender-systems-with-merlin-part-1-89fabe2fa05b\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  # Specify GPU device IDs\n",
    "    protocol = \"tcp\"  # \"tcp\" or \"ucx\"\n",
    "    num_gpus = 1\n",
    "    NUM_GPUS=[0]\n",
    "    cores = multiprocessing.cpu_count() - 1 # Count the number of cores in a computer\n",
    "    visible_devices = \",\".join([str(n) for n in NUM_GPUS])  # Select devices to place workers\n",
    "    device_limit_frac = 0.7  # Spill GPU-Worker memory to host at this limit.\n",
    "    device_pool_frac = 0.8\n",
    "    part_mem_frac = 0.15\n",
    "\n",
    "    # Manually specify the total device memory size (in bytes)\n",
    "    device_size = 10 * 1024 * 1024 * 1024  # GPU has 12GB but setting at 10GB\n",
    "            \n",
    "    ram_memory_limit = \"75GB\" # Set the RAM memory limit (per worker)\n",
    "    device_limit = int(device_limit_frac * device_size)\n",
    "    device_pool_size = int(device_pool_frac * device_size)\n",
    "    part_size = int(part_mem_frac * device_size)\n",
    "\n",
    "    cluster = LocalCluster(\n",
    "            n_workers=(multiprocessing.cpu_count()-2),\n",
    "            threads_per_worker=2,\n",
    "            #processes=False,\n",
    "            memory_limit=ram_memory_limit,\n",
    "            local_directory=DASK_DIR,\n",
    "            dashboard_address=\":8787\",\n",
    "            protocol=\"tcp\",\n",
    "    )\n",
    "\n",
    "\n",
    "    # Create the distributed client\n",
    "    client = Client(cluster)\n",
    "\n",
    "    # Get information about workers from scheduler\n",
    "    workers_info = client.scheduler_info()[\"workers\"]\n",
    "\n",
    "    # Iterate over workers and set their memory limits\n",
    "    for worker_id, worker_info in workers_info.items():\n",
    "        worker_info[\"memory_limit\"] = ram_memory_limit\n",
    "\n",
    "    # Verify that memory limits have been set correctly\n",
    "    #for worker_id, worker_info in workers_info.items():\n",
    "    #    print(f\"Worker {worker_id}: Memory Limit - {worker_info['memory_limit']}\")\n",
    "\n",
    "    # verify that Dask is being used in your code, you can check the following:\n",
    "    # Check if the Dask client is connected to a scheduler:\n",
    "    if client.status == \"running\":\n",
    "        print(\"Dask client is connected to a scheduler.\")\n",
    "        # Scatter the embedding vectors across Dask workers\n",
    "    else:\n",
    "        print(\"Dask client is not connected to a scheduler.\")\n",
    "\n",
    "    # Check if Dask workers are running:\n",
    "    if len(client.scheduler_info()[\"workers\"]) > 0:\n",
    "        print(\"Dask workers are running.\")\n",
    "    else:\n",
    "        print(\"No Dask workers are running.\")\n",
    "\n",
    "    # Load data for MIN_YEAR\n",
    "    with open(f\"C:/_harvester/data/tokenized-sentences/10s/{MIN_YEAR}-tokenized_sents-w-bigrams.json\", \"r\") as fp:\n",
    "        min_year_data = json.load(fp)\n",
    "\n",
    "    # Check the data format and structure for MIN_YEAR\n",
    "    print(f\"Data format and structure for {MIN_YEAR}:\")\n",
    "    print(f\"Type of min_year_data: {type(min_year_data)}\")\n",
    "    print(f\"Length of min_year_data: {len(min_year_data)}\")\n",
    "    # Add any additional checks specific to your data\n",
    "\n",
    "    # Loop over subsequent years (excluding MIN_YEAR)\n",
    "    for year in range(MIN_YEAR + 1, MAX_YEAR):\n",
    "        # Load data for the current year\n",
    "        with open(f\"C:/_harvester/data/tokenized-sentences/10s/{year}-tokenized_sents-w-bigrams.json\", \"r\") as fp:\n",
    "            current_year_data = json.load(fp)\n",
    "\n",
    "        # Check the data format and structure for the current year\n",
    "        print(f\"\\nData format and structure for {year}:\")\n",
    "        print(f\"Type of current_year_data: {type(current_year_data)}\")\n",
    "        print(f\"Length of current_year_data: {len(current_year_data)}\")\n",
    "        # Add any additional checks specific to your data\n",
    "\n",
    "        # Compare the data format and structure between MIN_YEAR and the current year\n",
    "        if type(min_year_data) != type(current_year_data):\n",
    "            print(\"Data types are different.\")\n",
    "            sys.exit()  # Stop execution after encountering this condition\n",
    "        \n",
    "        #if len(min_year_data) != len(current_year_data):\n",
    "        #    print(\"Data lengths are different.\")\n",
    "        #    sys.exit()  # Stop execution after encountering this condition\n",
    "\n",
    "\n",
    "\n",
    "    #@dask.delayed\n",
    "    def train_model(n_topics, alpha, beta):\n",
    "        #dictionary = Dictionary()  # Create an empty dictionary\n",
    "        combined_corpus = []  # Initialize list to store combined corpus\n",
    "        combined_text = []\n",
    "\n",
    "        passes = 11  # Number of passes\n",
    "\n",
    "        #print(\"We are before the loop.\")\n",
    "        for year in tqdm(range(MIN_YEAR, MAX_YEAR), desc=\"Training LDA models\"):\n",
    "            #print(f\"This is the year value that is extracted from the Range {year}\")\n",
    "            with open(f\"C:/_harvester/data/tokenized-sentences/10s/{year}-tokenized_sents-w-bigrams.json\", \"r\") as fp:\n",
    "                texts_out =  json.load(fp)\n",
    "\n",
    "            dictionary = Dictionary(texts_out)\n",
    "\n",
    "            if len(dictionary) > 0:\n",
    "                corpus = [dictionary.doc2bow(doc) for doc in texts_out]\n",
    "\n",
    "                perplexity_logger = PerplexityMetric(corpus=corpus, logger='shell')\n",
    "                if year == MIN_YEAR:\n",
    "                    logging.info(f\"Training the initial model on a single corpus for year {year}.\\n\")\n",
    "                    lda_model_gensim = LdaModel(corpus=corpus,\n",
    "                                                id2word=dictionary,\n",
    "                                                num_topics=n_topics,\n",
    "                                                alpha=alpha,\n",
    "                                                eta=beta,\n",
    "                                                random_state=75,\n",
    "                                                passes=passes,\n",
    "                                                iterations=150,\n",
    "                                                #workers = cores,\n",
    "                                                chunksize=4000,\n",
    "                                                per_word_topics=True,\n",
    "                                                #callbacks=[pbar]\n",
    "                                            )\n",
    "\n",
    "                else:\n",
    "                    logging.info(f\"Updating the model with year {year} data.\\n\")\n",
    "                    lda_model_gensim.update(corpus)\n",
    "\n",
    "                combined_text += texts_out\n",
    "                dictionary.add_documents(texts_out)  # Update the dictionary with new documents\n",
    "                combined_corpus.extend(corpus)  # Extend the combined corpus with current year's corpus\n",
    "                \n",
    "                # Convert tensors to strings in combined_text\n",
    "                #documents = [[str(w.item()) if isinstance(w, torch.Tensor) else str(w) for w in doc] for doc in combined_text]\n",
    "\n",
    "                # Create a new dictionary using modified documents\n",
    "                dictionary = Dictionary(combined_text)\n",
    "        return lda_model_gensim, combined_corpus, combined_text, dictionary\n",
    "    \n",
    "        \n",
    "    results = []\n",
    "    corpus_output = []\n",
    "\n",
    "    # Calculate the total number of iterations for the progress bar\n",
    "    total_iterations = len(range(start_topics, end_topics + 1, step_size)) * len(alpha_values) * len(beta_values)\n",
    "\n",
    "    print(\"Training LDA models.\")\n",
    "    for n_topics in range(start_topics, end_topics + 1, step_size):\n",
    "        topics_message = f\"Topics({n_topics}) are in the model being trained.\"\n",
    "        for alpha, beta in itertools.product(alpha_values, beta_values):\n",
    "            # Submit train_model function as a task to Dask cluster and get future object\n",
    "            future = dask.delayed(train_model)(n_topics, alpha, beta)\n",
    "            results.append(future)\n",
    "        \n",
    "        print(topics_message)\n",
    "    print(\"\\n\")\n",
    "\n",
    "    # Compute lda_models using Dask\n",
    "    with dask.config.set(scheduler='distributed'):\n",
    "        try:\n",
    "            lda_models = dask.compute(*results, progressbar=True)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(\"\\n\")\n",
    "            for result in results:\n",
    "                if not isinstance(result, dask.delayed.Delayed):\n",
    "                    print(\"Invalid element found in results:\", result)\n",
    "\n",
    "    progress_bar = tqdm(total=len(lda_models), desc=\"Calculating metrics\")\n",
    "    for (lda_model_gensim, combined_corpus, combined_text, dictionary), (alpha, beta) in zip(lda_models, itertools.product(alpha_values, beta_values)):\n",
    "        n_topics = start_topics + step_size * lda_models.index((lda_model_gensim, combined_corpus, combined_text, dictionary))\n",
    "    \n",
    "        # Compute convergence score\n",
    "        convergence_score = lda_model_gensim.bound(combined_corpus)\n",
    "\n",
    "        # Compute perplexity score\n",
    "        perplexity_score = lda_model_gensim.log_perplexity(combined_corpus)\n",
    "\n",
    "        # Compute coherence score\n",
    "        c_v_score = coherence_score(X=combined_text, n_topics=n_topics,\n",
    "                                    vectorizer='glove', glove=glove)\n",
    "\n",
    "        # Save the Gensim LDA model\n",
    "        print(\"Saving the Gensim LDA model.\")\n",
    "        best_model_gensim_filename = os.path.join(model_dir, f\"topics({n_topics})_alpha({alpha})_beta({beta}).model\")\n",
    "        lda_model_gensim.save(best_model_gensim_filename)\n",
    "\n",
    "        # Generate and save a visualization for the Gensim LDA model\n",
    "        #pyLDAvis.enable_notebook()\n",
    "        #try:\n",
    "        #    vis_data = pyLDAvis.gensim.prepare(lda_model_gensim, combined_corpus, dictionary)\n",
    "        #except AssertionError:\n",
    "        #    vis_data = None\n",
    "\n",
    "        #if vis_data is not None:\n",
    "        #    vis_html_filename = os.path.join(image_dir, f\"topics({n_topics})_alpha({alpha})_beta({beta}).html\")\n",
    "        #    pyLDAvis.save_html(vis_data, vis_html_filename)\n",
    "\n",
    "        # Add metrics to dictionary\n",
    "        metrics_csv['n_topics'].append(n_topics)\n",
    "        metrics_csv['alpha'].append(alpha)\n",
    "        metrics_csv['beta'].append(beta)\n",
    "        metrics_csv['cv_score'].append(c_v_score)\n",
    "        metrics_csv['convergence_score'].append(convergence_score)\n",
    "        metrics_csv['log_perplexity'].append(perplexity_score)\n",
    "\n",
    "        # Log metrics to a file\n",
    "        log_filename_txt = os.path.join(log_dir, \"lda_metrics.txt\")\n",
    "\n",
    "        with open(log_filename_txt, 'a') as log_file:\n",
    "            for (n_topics, alpha, beta) in itertools.product(range(start_topics,end_topics+1), alpha_values,beta_values):\n",
    "                        log_file.write(f\"Number of Topics: {n_topics}  |  \")\n",
    "                        log_file.write(f\"Alpha: {alpha}  |  \")\n",
    "                        log_file.write(f\"Beta: {beta}  |  \")\n",
    "                        log_file.write(f\"Coherence Value (c_v) - Gensim: {c_v_score}  |  \")\n",
    "                        log_file.write(f\"Convergence Score - Gensim: {convergence_score}  |  \")\n",
    "                        log_file.write(f\"Log Perplexity - Gensim: {perplexity_score}\\n\")\n",
    "\n",
    "        progress_bar.update(1)\n",
    "    progress_bar.close()\n",
    "    pd.DataFrame(metrics_csv).to_pickle('C:/_harvester/data/lda-models/2010s_html/2010s-lda_tuning_results.pkl')\n",
    "    pd.DataFrame(metrics_csv).to_csv('C:/_harvester/data/lda-models/2010s_html/2010s-lda_tuning_results.csv', index=False)   \n",
    "\n",
    "    # Close the Dask client and cluster when done\n",
    "    client.close()\n",
    "    #cluster.close(timeout=60)\n",
    "    cluster.close()\n",
    "    logging.shutdown()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close the Dask client and cluster when done\n",
    "client.close()\n",
    "cluster.close(timeout=60)\n",
    "logging.shutdown()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tsne_plot(topic_word_distributions, feature_names, n_topics):\n",
    "    \"\"\"\n",
    "    Generates a t-SNE plot for the given topic-word distributions.\n",
    "    \n",
    "    Args:\n",
    "        topic_word_distributions (ndarray): Topic-word distributions from LDA model.\n",
    "        feature_names (list): List of feature names from CountVectorizer.\n",
    "        n_topics (int): Number of topics in LDA model.\n",
    "    \"\"\"\n",
    "    tsne = TSNE(n_components=2)\n",
    "    tsne_results = tsne.fit_transform(topic_word_distributions.T)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    for i in range(n_topics):\n",
    "        plt.scatter(tsne_results[:, 0], tsne_results[:, 1], label=f\"Topic {i+1}\")\n",
    "        \n",
    "        for j, txt in enumerate(feature_names):\n",
    "            plt.annotate(txt, (tsne_results[j, 0], tsne_results[j, 1]))\n",
    "            \n",
    "    plt.title(\"t-SNE Plot of Topic-Word Distributions\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_word_cloud(topic_distribution, feature_names, topic_idx):\n",
    "   \"\"\"\n",
    "   Generates a word cloud based on the given topic distribution and feature names.\n",
    "\n",
    "   Args:\n",
    "       topic_distribution (ndarray): Topic distribution from LDA model.\n",
    "       feature_names (list): List of feature names from CountVectorizer.\n",
    "       topic_idx (int): Index of the topic.\n",
    "   \"\"\"\n",
    "   # Create a dictionary of words and their corresponding weights in the topic distribution\n",
    "   word_weights = {feature_names[i]: weight for i, weight in enumerate(topic_distribution)}\n",
    "\n",
    "   # Generate word cloud visualization\n",
    "   wc = WordCloud(background_color='white')\n",
    "   wc.generate_from_frequencies(word_weights)\n",
    "\n",
    "   # Plot the word cloud\n",
    "   plt.figure(figsize=(8, 6))\n",
    "   plt.imshow(wc, interpolation='bilinear')\n",
    "   plt.axis('off')\n",
    "   plt.title(f\"Word Cloud for Topic {topic_idx + 1}\")\n",
    "   plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from gensim.models import LdaModel\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "def compare_models_sklearn_gensim(sklearn_models, gensim_models, data):\n",
    "    \"\"\"\n",
    "    Compares scikit-learn's LatentDirichletAllocation (LDA) models with gensim's LdaModel.\n",
    "    \n",
    "    Args:\n",
    "        sklearn_models (list): List of scikit-learn LDA models.\n",
    "        gensim_models (list): List of gensim LdaModel.\n",
    "        data (list): List of tokenized sentences.\n",
    "    \"\"\"\n",
    "    # Convert tokenized sentences to text documents by joining tokens with space separator\n",
    "    documents = [' '.join(tokens) for tokens in data]\n",
    "\n",
    "    # Convert text data to numerical representation using CountVectorizer\n",
    "    vectorizer = CountVectorizer()\n",
    "    X = vectorizer.fit_transform(documents)\n",
    "\n",
    "    # Create a PyTorch tensor from the sparse matrix and move it to the device\n",
    "    X_tensor = torch.from_numpy(X.toarray()).float()\n",
    "\n",
    "    # Create a Gensim Dictionary from the tokenized sentences\n",
    "    dictionary = Dictionary(data)\n",
    "    \n",
    "    for i, (sk_model, gs_model) in enumerate(zip(sklearn_models, gensim_models)):\n",
    "        print(f\"Comparison for Model {i+1}:\")\n",
    "        \n",
    "        # Compare coherence values using Gensim's CoherenceModel\n",
    "        coherence_sk = sk_model.score(X)\n",
    "        \n",
    "        pbar = tqdm(total=len(data), desc=\"Calculating Coherence Value - Gensim\")\n",
    "        coherence_gs = 0\n",
    "        \n",
    "        for doc in data:\n",
    "            bow = dictionary.doc2bow(doc)\n",
    "            coherence_gs += gs_model.log_perplexity([bow])\n",
    "            pbar.update(1)\n",
    "        \n",
    "        pbar.close()\n",
    "        \n",
    "        coherence_gs /= len(data)\n",
    "        \n",
    "        print(f\"Coherence Value - scikit-learn: {coherence_sk}\")\n",
    "        print(f\"Coherence Value - Gensim: {coherence_gs}\\n\")\n",
    "\n",
    "# Example usage:\n",
    "sklearn_models = [lda_model_100_topics, lda_model_200_topics]\n",
    "gensim_models = [lda_gensim_100_topics, lda_gensim"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
